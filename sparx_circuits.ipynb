{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ki9DQSKUEAqB"
      },
      "source": [
        "# Comparing circuit and QAF explanations in MLPs\n",
        "\n",
        "For a given MLP, we aim to compare explanations for its behaviour.\n",
        "We compare circuits identified using activation patching, and mechanisms identified using SpArX QAFs.\n",
        "\n",
        "1. Train an MLP on the COMPAS dataset.\n",
        "2. Use SpArX to produce a QAF explaining the MLP.\n",
        "3. Use activation patching to quantify the importance of each neuron in the MLP with respect to the output. (Sweep over all neurons in the hidden layers and measure effect on output using logit differences.)\n",
        "\n",
        "Potential outcomes:\n",
        "\n",
        "* SpArX circuits are formed by averaging weights of clustered neurons\n",
        "* Activation patching scores are based on logit differences after ablation\n",
        "* Weights and activation values are both measures of a neuron's importance\n",
        "\n",
        "* Can we cluster neurons based on their logit differences (importance scores)? Does this correspond to SpArX?\n",
        "* "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-03-04 09:57:53.163264: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2025-03-04 09:57:53.166276: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2025-03-04 09:57:53.174937: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1741082273.189976  318423 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1741082273.194645  318423 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-04 09:57:53.213276: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tf_keras as keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "L44ZpT6VNku4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tf_keras.models import Sequential\n",
        "from tf_keras.layers import Dense\n",
        "from SpArX.sparx import FFNN, KMeansClusterer, GlobalMerger, BokehVisualizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1_e_0Kvn0HL"
      },
      "source": [
        "## Set up dataset and MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-v9_uCiKtJJk"
      },
      "outputs": [],
      "source": [
        "def summarise_charge(x):\n",
        "    drugs = ['clonaz', 'heroin', 'cocaine', 'cannabi', 'drug', 'pyrrolidin', 'Methyl', 'MDMA', 'Ethylone',\n",
        "              'Alprazolam', 'Oxycodone',\n",
        "              'Methadone', 'Methamph', 'Bupren', 'Lorazepam', 'controlled', 'Amphtamine', 'contro', 'cont sub',\n",
        "              'rapher', 'fluoro',\n",
        "              'ydromor', 'methox', 'iazepa', 'XLR11', 'steroid', 'morphin', 'contr sub', 'enzylpiper', 'butanediol',\n",
        "              'phentermine',\n",
        "              'Fentanyl', 'Butylone', 'Hydrocodone', 'LSD', 'Amobarbital', 'Amphetamine', 'Codeine', 'Carisoprodol']\n",
        "    drugs_selling = ['sel', 'del', 'traf', 'manuf']\n",
        "    if sum([d.lower() in x.lower() for d in drugs]) > 0:\n",
        "        if sum([h in x.lower() for h in drugs_selling]) > 0:\n",
        "            x = 'Drug Traffic'\n",
        "        else:\n",
        "            x = 'Drug Possess'\n",
        "    elif 'murd' in x.lower() or 'manslaughter' in x.lower():\n",
        "        x = 'Murder'\n",
        "    elif 'sex' in x.lower() or 'porn' in x.lower() or 'voy' in x.lower() or 'molest' in x.lower() or 'exhib' in x.lower():\n",
        "        x = 'Sex Crime'\n",
        "    elif 'assault' in x.lower() or 'carjacking' in x.lower():\n",
        "        x = 'Assault'\n",
        "    elif 'child' in x.lower() or 'domestic' in x.lower() or 'negle' in x.lower() or 'abuse' in x.lower():\n",
        "        x = 'Family Crime'\n",
        "    elif 'batt' in x.lower():\n",
        "        x = 'Battery'\n",
        "    elif 'burg' in x.lower() or 'theft' in x.lower() or 'robb' in x.lower() or 'stol' in x.lower():\n",
        "        x = 'Theft'\n",
        "    elif 'fraud' in x.lower() or 'forg' in x.lower() or 'laund' in x.lower() or 'countrfeit' in x.lower() or 'counter' in x.lower() or 'credit' in x.lower():\n",
        "        x = 'Fraud'\n",
        "    elif 'prost' in x.lower():\n",
        "        x = 'Prostitution'\n",
        "    elif 'trespa' in x.lower() or 'tresspa' in x.lower():\n",
        "        x = 'Trespass'\n",
        "    elif 'tamper' in x.lower() or 'fabricat' in x.lower():\n",
        "        x = 'Tampering'\n",
        "    elif 'firearm' in x.lower() or 'wep' in x.lower() or 'wea' in x.lower() or 'missil' in x.lower() or 'shoot' in x.lower():\n",
        "        x = 'Firearm'\n",
        "    elif 'alking' in x.lower():\n",
        "        x = 'Stalking'\n",
        "    elif 'dama' in x.lower():\n",
        "        x = 'Damage'\n",
        "    elif 'driv' in x.lower() or 'road' in x.lower() or 'speed' in x.lower() or 'dui' in x.lower() or 'd.u.i.' in x.lower():\n",
        "        x = 'Driving'\n",
        "\n",
        "    else:\n",
        "        x = 'Other'\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "0qwg5aYqoG6a",
        "outputId": "d2dc43b2-c728-47c8-ac1e-3c1aaf29be6f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from datetime import datetime\n",
        "\n",
        "# Load the COMPAS dataset\n",
        "url = \"https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Select relevant features and target variable\n",
        "df['age'] = df['age_cat']\n",
        "df['charge_desc'] = df['c_charge_desc']\n",
        "df['charge_degree'] = df['c_charge_degree']\n",
        "\n",
        "features = ['sex', 'age', 'race', 'decile_score', 'out_custody', 'in_custody',\n",
        "            'juv_fel_count', 'juv_misd_count', 'juv_other_count',\n",
        "            'priors_count', 'days_b_screening_arrest', 'charge_degree', 'charge_desc',\n",
        "            'is_recid', 'is_violent_recid', 'priors_count']\n",
        "target = 'two_year_recid'\n",
        "\n",
        "df = df[features + [target]]\n",
        "# Drop rows with missing values\n",
        "df = df.dropna()\n",
        "df['charge_desc'] = df['charge_desc'].apply(summarise_charge)\n",
        "df['custody'] = (df['out_custody'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d')) - df['in_custody'].apply(\n",
        "        lambda x: datetime.strptime(x, '%Y-%m-%d'))).apply(lambda x: x.total_seconds() / 3600 / 24).astype(int)\n",
        "del df['out_custody']\n",
        "del df['in_custody']\n",
        "\n",
        "features.remove('out_custody')\n",
        "features.remove('in_custody')\n",
        "features.append('custody')\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Loading the trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The model might have already been trained - in which case do not run the cells below this\n",
        "from keras.models import load_model\n",
        "\n",
        "model = load_model('compas_model.keras')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KBkqAuqs7OY",
        "outputId": "abbf943e-2fa8-46d9-b2b4-2dd46006e2ff"
      },
      "outputs": [],
      "source": [
        "# Split the data into features and target\n",
        "X = df[features]\n",
        "y = df[target]\n",
        "\n",
        "# One-hot\n",
        "X = pd.get_dummies(X, drop_first=True)\n",
        "y = pd.get_dummies(y, drop_first=True)\n",
        "\n",
        "# Randomize\n",
        "X = X.sample(frac=1, random_state=2020)\n",
        "y = y.loc[X.index.values]\n",
        "X.reset_index(inplace=True, drop=True)\n",
        "y.reset_index(inplace=True, drop=True)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zl5Kfm6dxZHd",
        "outputId": "34a6a854-cd6b-4e2d-8a9a-efdc6c47cc9c"
      },
      "outputs": [],
      "source": [
        "from keras.metrics import Accuracy, Precision, Recall\n",
        "\n",
        "input_size = len(X.columns.values)\n",
        "output_size = len(y.columns.values)\n",
        "\n",
        "# Build the model\n",
        "model = Sequential([\n",
        "    Dense(50, activation='relu', input_shape=(input_size,)),\n",
        "    Dense(50, activation='relu'),\n",
        "    Dense(output_size, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[Accuracy(), Precision(), Recall()], run_eagerly=True)\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxccF2izn-Iq",
        "outputId": "549ace91-28e4-4aa9-c0f6-7a7f34371ccd"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=64, validation_data=(X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the trained model\n",
        "model.save(\"compas_model.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "id": "GoNgK7a0zvtT",
        "outputId": "feb64334-f5b8-456c-83a9-de86882e621c"
      },
      "outputs": [],
      "source": [
        "score = model.evaluate(X_test, y_test)\n",
        "plt.figure(figsize=(14, 6))\n",
        "for key in history.history.keys():\n",
        "    plt.plot(history.history[key], label=key)\n",
        "plt.legend(loc='best')\n",
        "plt.grid(alpha=.2)\n",
        "plt.title(f'batch_size = 64, epochs = 100')\n",
        "plt.draw()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUoxv6RJn6u6"
      },
      "source": [
        "# Perform SpArX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48n4g8QhnHyw"
      },
      "outputs": [],
      "source": [
        "# shrink to a decimal percentage\n",
        "SHRINK_TO_PERCENTAGE = 0.15\n",
        "\n",
        "shape = (input_size, 50, 50, output_size)\n",
        "weights = [layer.get_weights()[0] for layer in model.layers]\n",
        "bias = [layer.get_weights()[1] for layer in model.layers]\n",
        "activations = [\"relu\", \"relu\", \"sigmoid\"]\n",
        "\n",
        "restored_model = FFNN(shape, weights, bias, activations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBAlZ5HT2Zp5",
        "outputId": "8a928dab-3ff8-4c19-b3f4-06ead46f34fe"
      },
      "outputs": [],
      "source": [
        "restored_model.forward_pass(X_test)\n",
        "\n",
        "\n",
        "# cluster into 2 clusters\n",
        "cluster_labels = KMeansClusterer.cluster(restored_model, SHRINK_TO_PERCENTAGE)\n",
        "\n",
        "print(cluster_labels)\n",
        "\n",
        "# merge clusters\n",
        "merged_model = GlobalMerger.merge(restored_model, cluster_labels)\n",
        "restored_model.model.summary()\n",
        "merged_model.model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJroqJky2o4N",
        "outputId": "05241539-9fa2-425a-f0e9-82ec85b01afd"
      },
      "outputs": [],
      "source": [
        "# Measure fidelity\n",
        "\n",
        "def infidelity(x, y):\n",
        "  return np.linalg.norm(x - y)/len(x)\n",
        "\n",
        "restored_model.forward_pass(X_test)\n",
        "original_output = restored_model.forward_pass_data[-1]\n",
        "\n",
        "merged_model.forward_pass(X_test)\n",
        "merged_output = merged_model.forward_pass_data[-1]\n",
        "\n",
        "print(infidelity(original_output, merged_output))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "C8iFOBdwJpgS",
        "outputId": "3d8803e4-f8b5-459c-bc44-a49d6b83a97e"
      },
      "outputs": [],
      "source": [
        "from bokeh.io import output_notebook\n",
        "output_notebook()\n",
        "\n",
        "BokehVisualizer.visualise(merged_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVSzVk-BMNQZ"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.keras import backend as K\n",
        "\n",
        "def general_method_for_visualize_attack_and_supports_QBAF(input, output, model, feature_names, number_of_hidden_nodes,\n",
        "                                                          quantile, weights, biases, path, fig_index):\n",
        "\n",
        "    activation = compute_activations_for_each_layer(model, input)\n",
        "    output_activation_in_correct_format = list(map(int, activation[-1][0] > 0.5))\n",
        "    # import module\n",
        "    from graphviz import Digraph\n",
        "    import numpy as np\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    # instantiating object\n",
        "    dot = Digraph(comment='A Round Graph')\n",
        "    dot.attr(rankdir=\"LR\")\n",
        "    dot.attr(splines='line')\n",
        "    dot.subgraph()\n",
        "\n",
        "\n",
        "    existing_nodes = []\n",
        "    input = input.reshape(-1,)\n",
        "    # Adding nodes\n",
        "    for layer_index in range(len(number_of_hidden_nodes) + 2):  # +2 input and output layers\n",
        "        if layer_index <= len(number_of_hidden_nodes):\n",
        "          edge_weight_threshold = np.quantile(np.abs(np.array(weights[layer_index])).reshape(1, -1), quantile)\n",
        "        existing_nodes.append([])\n",
        "        if layer_index == 0:\n",
        "            for i in range(len(input)):\n",
        "                if input[i] != 0:\n",
        "                    if  max(np.abs(np.array(weights[layer_index])[i, :])) >= edge_weight_threshold:\n",
        "                        dot.node('I' + str(i), str(feature_names[i]))\n",
        "                        existing_nodes[layer_index].append(i)\n",
        "        elif layer_index == len(number_of_hidden_nodes) + 1:\n",
        "            for i in range(len(output)):\n",
        "                dot.node('O' + str(i), f'<O<SUB>{str(i)}</SUB>>', #color='green' if activation[-1] == list(output) else 'red',\n",
        "                         width=str(np.abs(activation[-1].reshape((-1,))[i]) * 2))\n",
        "                existing_nodes[layer_index].append(i)\n",
        "        else:\n",
        "            dot.attr('node', shape='circle')\n",
        "            for i in range(number_of_hidden_nodes[layer_index - 1]):\n",
        "                if max(np.abs(np.array(weights[layer_index])[i, :])) >= edge_weight_threshold or \\\n",
        "                        np.max(np.abs(np.array(weights[layer_index - 1])[existing_nodes[layer_index - 1], i])) >= edge_weight_threshold:\n",
        "                    dot.node('H' + str(layer_index) + \".\" + str(i), f\"<C<SUB>{str(i +1)}</SUB>>\")#<SUP>({str(layer_index)})</SUP>>\")#, #color='green' if np.sum(weights[layer_index][i]) > 0 else 'red',\n",
        "                             #width=str(np.abs(activation[layer_index-1].reshape((-1,))[i])))\n",
        "                    existing_nodes[layer_index].append(i)\n",
        "\n",
        "    # dot.node('Explanations', label='Ground Truth: ' + str(output) +\n",
        "    #                                '\\\\nPrediction:' + str(output_activation_in_correct_format) + ' \\\\n Output activation value: '\n",
        "    #                                + f'activation:.{activation[-1][0]}', shape='note')\n",
        "    omitted_edges = []\n",
        "    for layer_index in range(len(number_of_hidden_nodes) + 1):\n",
        "            edge_weight_threshold = np.quantile(np.abs(np.array(weights[layer_index])).reshape(1, -1), quantile)\n",
        "            omitted_edges.append([])\n",
        "            if layer_index == 0:\n",
        "                for i in existing_nodes[layer_index]:\n",
        "                    for j in existing_nodes[layer_index + 1]:\n",
        "                        if np.abs(weights[layer_index][i, j]) >= edge_weight_threshold:\n",
        "                            dot.edge('I' + str(i), 'H' + str(layer_index + 1)+\".\" + str(j),\n",
        "                                     color='green' if weights[layer_index][i, j] >= 0 else 'red',\n",
        "                                     penwidth=str(np.abs( weights[layer_index][i, j]) * 2))\n",
        "                        else:\n",
        "                            omitted_edges[layer_index].append(f'{i}-{j}')\n",
        "            elif layer_index == len(number_of_hidden_nodes):\n",
        "                for i in existing_nodes[layer_index]:\n",
        "                    for j in existing_nodes[layer_index + 1]:\n",
        "                        if np.abs(weights[layer_index][i, j]) >= edge_weight_threshold and \\\n",
        "                                np.max(np.abs(weights[layer_index][existing_nodes[layer_index], j])) > edge_weight_threshold:\n",
        "                            dot.edge('H'+str(layer_index)+\".\" + str(i), 'O' + str(j),\n",
        "                                     color='green' if weights[layer_index][i, j] >= 0 else 'red',\n",
        "                                     penwidth=str(np.abs(weights[layer_index][i, j]) * 2))\n",
        "                        else:\n",
        "                            omitted_edges[layer_index].append(f'{i}-{j}')\n",
        "            else:\n",
        "                dot.attr('node', shape='circle')\n",
        "                for i in existing_nodes[layer_index]:\n",
        "                    for j in existing_nodes[layer_index + 1]:\n",
        "                        if np.abs(weights[layer_index][i, j]) >= edge_weight_threshold and \\\n",
        "                                np.max(np.abs(weights[layer_index][existing_nodes[layer_index], j])) >= edge_weight_threshold:\n",
        "                            dot.edge('H' + str(layer_index)+\".\"  + str(i), 'H' + str(layer_index + 1)+\".\" + str(j),\n",
        "                                     color='green' if weights[layer_index][i, j] >= 0 else 'red',\n",
        "                                     penwidth=str(np.abs(weights[layer_index][i, j]) * 2))\n",
        "                        else:\n",
        "                            omitted_edges[layer_index].append(f'{i}-{j}')\n",
        "\n",
        "    # for i in range(len(output)):\n",
        "    #     dot.edge('O' + str(i), 'Explanations', penwidth=str(0))\n",
        "\n",
        "    nodes_to_be_omitted = []\n",
        "    #remove unneccessary nodes\n",
        "    for layer_index in range(len(number_of_hidden_nodes)):\n",
        "        current_nodes_dictionary = {}\n",
        "        for index, node in enumerate(existing_nodes[layer_index]):\n",
        "            current_nodes_dictionary[node] = index\n",
        "        current_layer_nodes = [[] for i in range(len(existing_nodes[layer_index]))]\n",
        "        for omitted_edge in omitted_edges[layer_index]:\n",
        "            current_layer_nodes[current_nodes_dictionary[int(omitted_edge.split('-')[0])]].append(int(omitted_edge.split('-')[1]))\n",
        "        for index, nodes in enumerate(current_layer_nodes):\n",
        "            if nodes == existing_nodes[layer_index + 1]:\n",
        "                if layer_index == 0:\n",
        "                    nodes_to_be_omitted.append(f'I{existing_nodes[layer_index][index]}')\n",
        "                else:\n",
        "                    nodes_to_be_omitted.append(f'H{layer_index}.{existing_nodes[layer_index][index]}')\n",
        "\n",
        "    for node in nodes_to_be_omitted:\n",
        "        for body_node in dot.body:\n",
        "            if node in body_node:\n",
        "                dot.body.remove(body_node)\n",
        "    # saving source code\n",
        "    dot.format = 'png'\n",
        "    dot.render(path + '/Graph_' + str(fig_index))\n",
        "\n",
        "\n",
        "def compute_activations_for_each_layer(model, input_data):\n",
        "    inp = model.input  # input placeholder\n",
        "    outputs = [layer.output for layer in model.layers]  # all layer outputs\n",
        "    functor = K.function([inp], outputs)  # evaluation functions\n",
        "\n",
        "    # computing activations\n",
        "    activations = functor([input_data])\n",
        "    return activations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OND0Ahp3MvWp",
        "outputId": "a4aa9d48-4b9b-45f4-d9a7-751c54ef74e6"
      },
      "outputs": [],
      "source": [
        "# All weights and biases from merged model\n",
        "\n",
        "all_weights = []\n",
        "all_biases = []\n",
        "for layer in merged_model.model.layers:\n",
        "  w, b = layer.get_weights()\n",
        "  print(w.shape)\n",
        "  all_weights.append(w)\n",
        "  all_biases.append(b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_J2ZFOEPMA3n",
        "outputId": "e89dd419-2b5a-48c6-de53-e55480a7be25"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "for test_index in range(0, 20):\n",
        "  input = X_test[test_index].reshape(1,-1)\n",
        "  output = np.array(y_test)[test_index]\n",
        "  feature_names = X.columns.values\n",
        "  number_of_hidden_nodes = [8, 8]\n",
        "\n",
        "  general_method_for_visualize_attack_and_supports_QBAF(input, output, merged_model.model, feature_names, number_of_hidden_nodes,\n",
        "                                                        0.75, all_weights, all_biases, 'COMPAS_global', test_index)\n",
        "\n",
        "  # Show example\n",
        "  display(Image(filename=f'COMPAS_global/Graph_{test_index}.png'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3Y_rEVVN2bb"
      },
      "source": [
        "# Activation patching\n",
        "\n",
        "We use activation patching to causally trace a pathway of important activations, which explain how a decision is made on MLPs.\n",
        "\n",
        "Activation patching works by storing the activations on a clean run and a corrupted run, then substituting clean activations into the corrupted run to identify sufficient activations to recover the clean behaviour."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKy5aFEZ6ni_"
      },
      "outputs": [],
      "source": [
        "from keras.models import Model\n",
        "from keras import Input\n",
        "from keras.layers import Lambda\n",
        "\n",
        "def patch_neuron_activations(model, layer_index, neuron_index, original_input):\n",
        "    \"\"\"\n",
        "    Patches a specific neuron's activations in a layer.\n",
        "\n",
        "    Args:\n",
        "        model: The Keras model.\n",
        "        layer_index: Index of the layer to patch.\n",
        "        neuron_index: Index of the neuron to patch.\n",
        "        original_input: The original input to the model.\n",
        "\n",
        "    Returns:\n",
        "        The output of the model with patched neuron activations.\n",
        "    \"\"\"\n",
        "    # Get the activations for the original and patched inputs\n",
        "    layer_output = model.layers[layer_index].output\n",
        "    # Outputs the activations of the desired layer\n",
        "    intermediate_model = Model(inputs=model.input, outputs=layer_output)\n",
        "\n",
        "    original_activations = intermediate_model(original_input)\n",
        "\n",
        "    # Replace only the specified neuron's activations - set to zero (ablate)\n",
        "    patched_activations = original_activations.numpy()\n",
        "    patched_activations[0, neuron_index] = 0\n",
        "    patched_activations = tf.convert_to_tensor(patched_activations)\n",
        "\n",
        "    # Create a new model that patches the activations\n",
        "    inputs = Input(shape=original_input.shape[1:])\n",
        "    x = inputs\n",
        "\n",
        "    for i, layer in enumerate(model.layers):\n",
        "        if i == layer_index:\n",
        "            # Replace the activations with the patched ones\n",
        "            x = Lambda(lambda x: patched_activations)(x)\n",
        "        else:\n",
        "            x = layer(x)\n",
        "\n",
        "    patched_model = Model(inputs=inputs, outputs=x)\n",
        "\n",
        "    # Run the patched model\n",
        "    return patched_model(original_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Ne9lMoiApyy",
        "outputId": "4095f7e7-6e04-4f3e-f635-51813b0418d5"
      },
      "outputs": [],
      "source": [
        "# Select sample from dataset\n",
        "input_sample = X_test[0].reshape(1,-1)\n",
        "orig_output = model(input_sample)[0].numpy()\n",
        "print(orig_output)\n",
        "\n",
        "layer_logit_diffs = []\n",
        "\n",
        "# Loop over layers and neurons to identify critical ones\n",
        "for layer_index in range(len(model.layers) - 1):\n",
        "    if isinstance(model.layers[layer_index], Dense):\n",
        "        print(f\"Patching layer {layer_index}\")\n",
        "        logit_diffs = []\n",
        "        for neuron_index in range(model.layers[layer_index].units):\n",
        "            patched_output = patch_neuron_activations(model, layer_index, neuron_index, input_sample)\n",
        "            patched_output = patched_output[0].numpy()\n",
        "            logit_diff = np.abs(orig_output - patched_output)\n",
        "            print(f\"Neuron {neuron_index}: {patched_output}, logit diff {logit_diff}\")\n",
        "            logit_diffs.extend(logit_diff)\n",
        "        layer_logit_diffs.append(logit_diffs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ng0FcyG2CrZB",
        "outputId": "1d02618f-2e80-4a17-ed7a-41203de8087f"
      },
      "outputs": [],
      "source": [
        "layer_neurons_by_importance = np.argsort(layer_logit_diffs, axis=1)[::-1]\n",
        "layer_neurons_by_importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCvp2G_BbW0t",
        "outputId": "9be733b7-8bea-45d3-8dd9-604fa7629a2c"
      },
      "outputs": [],
      "source": [
        "# Get which cluster each of these important layer neurons belong to\n",
        "\n",
        "for i in range(len(layer_neurons_by_importance)):\n",
        "  ranked_cluster_labels = cluster_labels[i][layer_neurons_by_importance[i]]\n",
        "  print(ranked_cluster_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cd6Dv7TOcwaK"
      },
      "outputs": [],
      "source": [
        "# TODO: justify your expected results, ideally with more formalism"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "fyp-venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
